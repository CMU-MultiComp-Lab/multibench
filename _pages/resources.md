---
layout: resources
permalink: /resources/
title: Resources
---

MultiBench datasets:

1. MUStARD: 

2. CMU-MOSI:

3. UR-FUNNY: Hasan et al., [UR-FUNNY: A multimodal language dataset for understanding humor](https://arxiv.org/abs/1904.06618), EMNLP 2019

4. CMU-MOSEI:

5. MIMIC: Johnson et al., [MIMIC-III, a freely accessible critical care database](https://pubmed.ncbi.nlm.nih.gov/27219127/), Nature Scientific Data 2016

6. MuJoCo PUSH: Lee et al., [Multimodal sensor fusion with differentiable filters](https://arxiv.org/abs/2010.13021), IROS 2020

7. Vision&Touch: Lee et al., [Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks](https://arxiv.org/abs/1810.10191), ICRA 2019
 
8. ENRICO: Leiva et al., [Enrico: A dataset for topic modeling of mobile UI designs](https://userinterfaces.aalto.fi/enrico/resources/enrico.pdf), MobileHCI 2020

9. MM-IMDb:

10. AV-MNIST:

11. Kinetics-400: Kay et al., [The kinetics human action video dataset](https://arxiv.org/abs/1705.06950), arXiv 2017

MultiZoo algorithms:

1.

2.

Other resources:

1. 
